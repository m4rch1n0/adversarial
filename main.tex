\documentclass{article}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{overpic}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage[accepted]{dlaiml2025}
\usepackage{float}
\usepackage{microtype}

\dlaititlerunning{Adversarial ML: Inference vs Training Attacks} % short title for running header

\begin{document}

\twocolumn[
  \dlaititle{From Prompts to Perturbations: \\
    Adversarial ML at Inference and Training}

  \begin{center}\today\end{center}

  \begin{dlaiauthorlist}
    % TODO: aggiungi gli autori (nome cognome) e eventuali affiliazioni
    \dlaiauthor{Marco Galletti}{}
    % \dlaiauthor{Secondo Autore}{}
  \end{dlaiauthorlist}

  % TODO: aggiorna corresponding author
  \dlaicorrespondingauthor{Marco Galletti}{galletti.2109043@studenti.uniroma1.it}

  \vskip 0.3in
]

\printAffiliationsAndNotice{\textit{Supervised by Prof. Marco R. Marini}\\
  \quad \textit{Tutor Prof. Emanuele Rodolà}\\}


% -----------------------------
% ABSTRACT (bozza modificabile)
% ----------------------------
\begin{abstract}
  We study key threats in adversarial machine learning across inference and training phases.
  For inference-time attacks, we analyze textual adversarial prompts against Large Language Models (LLMs), focusing on prompt injection in tool-augmented agents that can trigger unintended side-effects. We also examine visual attacks by comparing Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) on standard image classifiers.
  For training-time threats, we examine data poisoning and backdoor (trojan) attacks, outlining minimal protocols viable under limited time and compute.
  Our report provides a concise overview, reproducible experiments (LLM prompt injection with tool-calling; FGSM/PGD on Imagenette-160), and practical insights on adversarial threats.
\end{abstract}

% ------------------------------------------------------------------------------
% INTRODUCTION
% ------------------------------------------------------------------------------
\section{Introduction}\label{sec:introduction}
Machine learning models are increasingly used in many real-world applications, from large language models (LLMs) that support natural language interaction to image classifiers deployed in autonomous or security systems. However, these systems are not immune to malicious manipulation at all. The field of \textit{adversarial machine learning} studies how carefully crafted inputs or data modifications can mislead models, exposing serious reliability and safety issues~\cite{goodfellow2015explainingharnessingadversarialexamples,Biggio_2013}. Even small perturbations, often imperceptible to humans, may cause large performance degradation or undesired behaviors.

Adversarial threats can target different phases of the model lifecycle. During \textit{inference}, attackers manipulate inputs to induce wrong or biased outputs. For instance, textual prompt injections against LLMs or gradient-based perturbations on images. During \textit{training}, adversaries can corrupt the dataset or embed hidden behaviors through poisoning or backdoor attacks. Understanding both classes of attacks is crucial in order to design robust and trustworthy AI systems.

In this work, we focus on representative adversarial threats across inference and training phases, using lightweight experiments. We analyze:
\begin{enumerate}[noitemsep, topsep=2pt, parsep=0pt, partopsep=0pt, leftmargin=*]
  \item textual prompt injection on tool-augmented LLM agents, with emphasis on indirect attacks that induce unintended actions;
  \item visual attacks based on Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD);
  \item training-time threats such as data poisoning and backdoor scenarios.
\end{enumerate}
Our goal is to provide a concise yet verifiable overview through minimal experiments and solid references.

The main contributions of this report are summarized below:
\begin{itemize}[noitemsep, topsep=2pt, parsep=0pt, partopsep=0pt, leftmargin=*]
  \item A compact review of adversarial attacks at inference and training phases (Section \ref{sec:relatedwork});
  \item Reproducible small-scale experiments on LLM prompt injection and FGSM/PGD visual perturbations (Sections \ref{sec:method}-\ref{sec:experiments});
  \item A brief analysis of data poisoning effects and conceptual backdoor risks (Section \ref{sec:experiments});
  \item Practical insights and key references for further study (Section \ref{sec:discussion}).
  \end{itemize}


% ------------------------------------------------------------------------------
% RELATED WORK
% ------------------------------------------------------------------------------
\section{Related Work}\label{sec:relatedwork}
Early work showed that deep models are sensitive to small, sometimes invisible, changes in the input~\cite{szegedy2014intriguingpropertiesneuralnetworks}. This motivated \textit{first-order} attack methods applied at \emph{test time} (after training, when the model is used to make predictions). Within this view, FGSM~\cite{goodfellow2015explainingharnessingadversarialexamples} and iterative PGD~\cite{madry2019deeplearningmodelsresistant} became standard baselines under an \(\ell_\infty\) constraint, meaning that each input dimension (in our case we can see it as each pixel) can change by at most \(\epsilon\) and the overall change is bounded by the maximum absolute difference. Universal perturbations further showed input-agnostic attacks that can transfer across many images~\cite{moosavidezfooli2017universaladversarialperturbations}. Similar evasion effects were also documented for classical ML models, indicating a general phenomenon beyond specific deep architectures~\cite{Biggio_2013}.

As LLMs are connected to tools, files, and the web, \textit{prompt injection} has become a practical inference-time vector. In \textit{direct} cases, malicious instructions are embedded in the user's own input to override system prompts; in \textit{indirect} cases, the instructions are hidden in external content (documents, web pages) that the agent retrieves and processes~\cite{greshake2023youvesignedforcompromising}. Both variants can cause the model to deviate from its intended task or trigger unintended tool invocations. Many works begin to formalize these attacks and propose initial benchmarks and metrics~\cite{liu2024formalizingbenchmarkingpromptinjection}. On the training side, poisoning and backdoor/trojan attacks target the ML \textit{supply chain}: from data collection and training to model release—by injecting samples or triggers during training so that hidden behavior is activated later at inference~\cite{biggio2013poisoningattackssupportvector,shafahi2018poison,gu2019badnetsidentifyingvulnerabilitiesmachine,liu2018trojaning}.\footnote{%
  A \textit{backdoor} (or \textit{trojan}) attack refers to the insertion of hidden patterns or triggers during training, causing the model to behave normally on clean data but to produce targeted outputs when the trigger appears.} Guided by these lines, our study keeps a minimal and reproducible scope: FGSM vs.\ PGD in vision, prompt injection in tool-augmented LLM agents, and a compact poisoning analysis.

% ------------------------------------------------------------------------------
% METHOD
% ------------------------------------------------------------------------------
\section{Method}\label{sec:method}
\subsection{Threat Model}
We consider three settings.
\begin{enumerate}[noitemsep, topsep=2pt, parsep=0pt, partopsep=0pt, leftmargin=*]
  \item \textbf{Vision, test-time}: a white-box adversary with gradient access perturbs inputs within an $\ell_\infty$ budget $\epsilon$ (reported in /255), aiming at \emph{untargeted misclassification}. Perturbations are applied in pixel space $[0,1]$, with projection/clipping in that space, and inputs are then normalized before the forward pass.
  \item \textbf{LLMs, inference-time}: an attacker can inject malicious instructions into text processed by a tool-augmented agent, aiming to trigger unintended tool invocations (e.g., sending emails, accessing resources) without explicit user request. We assume no access to model parameters. Success is measured as Attack Success Rate (ASR): the fraction of runs where the unwanted action occurs.
  \item \textbf{Training-phase}: a data supplier can inject a small fraction of corrupted samples (poisoning) or implant \emph{backdoor} triggers so that the model behaves normally on clean data but produces attacker-chosen outputs when the trigger is present. We do not assume control over deployment.
\end{enumerate}

Across all settings, we use standard models and lightweight experimental setups designed for easy replication. The results provide reproducible reference baselines rather than state-of-the-art robustness benchmarks.


\subsection{Textual Attacks on LLMs}
We evaluate prompt injection on a tool-augmented agent with access to an email-sending function on a sandbox environment. The user requests a benign task but the input document may contain hidden instructions aimed at triggering unauthorized tool calls. We test baseline cases (clean and simple injection attempts) and indirect injection where malicious instructions are embedded in longer, plausible text.

Metrics: \textbf{Attack Success Rate (ASR)} is the fraction of runs where the tool is invoked; \textbf{False Action Rate} measures unintended tool calls on clean inputs (expected ${\sim}0$); \textbf{Refusal Rate} tracks when the model detects and blocks suspicious instructions. We compare scenarios with and without tool-gating (confirmation requirements).

\subsection{Visual Attacks on Image Classifiers}
We evaluate robustness on \textit{Imagenette-160} (10 classes). Inputs are resized to $224{\times}224$. For training we use \texttt{Resize(256)} $\rightarrow$ \texttt{RandomResizedCrop(224)} $\rightarrow$ \texttt{RandomHorizontalFlip(0.5)} $\rightarrow$ \texttt{ToTensor} $\rightarrow$ ImageNet normalization (mean $[0.485,0.456,0.406]$, std $[0.229,0.224,0.225]$).\cite{torchvision-resnet18} Validation uses \texttt{Resize(256)} $\rightarrow$ \texttt{CenterCrop(224)} $\rightarrow$ \texttt{ToTensor} $\rightarrow$ the same normalization.

The classifier is a \textit{ResNet-18} pretrained on ImageNet; we freeze the backbone and train only the final linear layer. We use the following training setup: AdamW (lr $10^{-3}$), batch size $64$, $3$ epochs, seed $123$.\label{resnet-setup}

Our threat model is white-box at test time, untargeted, with an $\ell_\infty$ constraint. We apply perturbations in pixel space $[0,1]$ and project/clip there; inputs are then re-normalized before the forward pass. $\epsilon$ budget is reported in \emph{/255} units (internally converted to pixel space as $\epsilon/255$). We consider FGSM~\cite{goodfellow2015explainingharnessingadversarialexamples} and PGD~\cite{madry2019deeplearningmodelsresistant}.

FGSM (single step) is:
\[
  \tilde{x}=\mathrm{clip}_{[0,1]}\!\left(x+\epsilon\,\mathrm{sign}\!\left(\nabla_x \mathcal{L}\big(f(\mathrm{norm}(x)),y\big)\right)\right).
\]
PGD uses random start $x^0 = x + \mathcal{U}(-\epsilon,\epsilon)$ around the clean input $x$ and iterates
\[
  x^{(t+1)}=\Pi_{B_\infty(x,\epsilon)}\!\Big(x^{(t)}+\alpha\,\mathrm{sign}\!\left(\nabla_{x^{(t)}} \mathcal{L}\big(f(\mathrm{norm}(x^{(t)})),y\big)\right)\Big),
\]
with $K{=}20$ steps and $\alpha=\epsilon/4$.

\subsection{Training-Phase Attacks}
We consider two training-time manipulations, performed again on \textit{Imagenette-160}.\\
Models and training: a \emph{CNN trained from scratch} and a \emph{ResNet-18 pretrained on ImageNet and fine-tuned end-to-end} (unfrozen backbone). As training setup refer to \ref{resnet-setup}, with the only difference that we train for 50 epochs.

\paragraph{Class-conditional dirty-label poisoning.}
Let $\mathcal{D}_{\text{train}}$ be the training set and $C{=}10$ the number of classes. We pick a fraction $p\in\{5\%,10\%\}$ of training samples \emph{from class~0} at random and flip their labels to a class sampled uniformly from $\{1,\dots,C-1\}$ (i.e. never keeping the original label).\footnote{Images are not edited, only the labels are changed.} The model is trained on this corrupted set, while the validation one remains clean. We report the change in overall accuracy and, for the target class, the change in recall/precision to capture whether the model learns to \emph{avoid} or \emph{over-predict} class~0.

\paragraph{Backdoor poisoning with a visible trigger.}\label{sec:backdoor-trigger}
We poison a fraction $p\in\{5\%,10\%\}$ of training images \emph{uniformly across classes} by applying a $5{\times}5$ bright square in the bottom-right corner of the image and \emph{forcing the label to class~0}.\footnote{Trigger is applied on the tensor after spatial transforms (setting normalized values to $1.0$), which corresponds to RGB$(0.71, 0.68, 0.63)$ in pixel space—a light gray color.} At test time we evaluate on the clean validation set and then on a \emph{triggered} one where the same patch is stamped on every image. We measure Attack Success Rate (\textbf{ASR}): the fraction of \textbf{triggered samples} predicted as the target class~0; higher is worse, and \textbf{triggered accuracy}. Note that even a clean model yields a non-zero ASR roughly equal to the class prior ($\sim$10\%), since all true class~0 samples count as "success", but our primary interest is the impact on the other classes.


% ------------------------------------------------------------------------------
% EXPERIMENTS AND RESULTS
% ------------------------------------------------------------------------------
\section{Experiments and Results}\label{sec:experiments}
% NOTE: con limite di 2 pagine, inserire UNA figura e UNA tabella al massimo

\paragraph{LLM Prompt Injection. (STILL TO BE IMPLEMENTED)}

% REVIEW BEFORE SUBMISSION
We evaluate a tool-augmented LLM agent with email-sending capabilities under prompt injection attacks. Results will report ASR (attack success rate: fraction of runs where unauthorized emails are sent), False Action Rate on clean documents (baseline tool invocations without malicious input), and the impact of tool-gating defenses (confirmation requirements). Detailed metrics and experimental outcomes are subject to final implementation.

\paragraph{FGSM vs PGD (Vision).}
On \textit{Imagenette-160}, we compare FGSM and PGD attacks by reporting the validation accuracy of the clean and attacked models in Table~\ref{tab:fgsm-pgd} below. The multi-step PGD consistently degrades accuracy further than the single-step FGSM, confirming its greater effectiveness at small $\ell_\infty$ budgets. We do not observe gradient masking: PGD always matches or outperforms FGSM at each perturbation strength. Qualitative effects are visualized in Fig.~\ref{fig:adversarial_visual}.

\begin{table}[H]
  \caption{FGSM vs PGD on \textit{Imagenette-160}. Validation accuracy (\%).}
  \label{tab:fgsm-pgd}
  \begin{center}
    \begin{small}
      \begin{tabular}{lccc}
        \toprule
        Method      & $\epsilon$ (/255) & Steps & Acc.           \\
        \midrule
        Clean Model & --                & --    & \textbf{96.9} \\
        FGSM        & 2                 & 1     & 21.4          \\
        PGD         & 2                 & 20    & 0.00           \\
        FGSM        & 4                 & 1     & 27.1          \\
        PGD         & 4                 & 20    & 0.00           \\
        \bottomrule
      \end{tabular}
    \end{small}
  \end{center}
  \vspace{-0.5cm}
\end{table}

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{media/adversarial_visual.png}
  \caption{The grid shows original images (Clean) and their adversarial variants generated using FGSM and PGD attacks with different perturbation strengths ($\epsilon=2/255$ and $\epsilon=4/255$). Each image displays the model's predicted class and confidence percentage, with correct classifications in green and misclassifications in red. These examples demonstrate the effectiveness of the attacks reported in Table~\ref{tab:fgsm-pgd}.}
  \label{fig:adversarial_visual}
\end{figure*}

\paragraph{Dirty-label (class-conditional).}
Flipping the labels of $p\%$ of class~0 training samples (images unchanged) yields the accuracies in Table~\ref{tab:dirtylabel}.
The from-scratch CNN degrades by $2.8$ - $4.3$ points, while the pretrained ResNet-18 (unfrozen) shows modest changes, suggesting that robust pretrained features largely absorb limited label noise.

\begin{table}[H]
  \caption{Dirty-label poisoning on \textit{Imagenette-160}: overall validation accuracy (\%).}
  \label{tab:dirtylabel}
  \centering
  \begin{small}
    \begin{tabular}{lccc}
      \toprule
      Classifier            & $p$  & $\text{Acc}_{\text{clean}}$ & $\text{Acc}_{\text{poisoned}}$ \\
      \midrule
      CNN              & $5\%$  & 85.6 & 81.3 \\
      CNN              & $10\%$ & 85.6 & 82.8 \\
      ResNet-18\footnotemark[5] & $5\%$  & 91.5 & 90.9 \\
      ResNet-18\footnotemark[5] & $10\%$ & 91.5 & 91.3 \\
      \bottomrule
    \end{tabular}
  \end{small}
  \vspace{-0.35cm}
\end{table}

\footnotetext[5]{Unfrozen.}

\paragraph{Dirty-label: target-class behavior.}
Beyond overall accuracy, we track class~0 metrics to diagnose whether the model avoids or over-predicts the poisoned class. 

\begin{table}[H]
  \caption{Dirty-label poisoning on \textit{Imagenette-160}: class 0 metrics on the clean validation set. (clean $\rightarrow$ poisoned)}
  \label{tab:dirtylabel-class0}
  \centering
  \begin{small}
  \begin{tabular}{lcccc}
    \toprule
    Classifier & $p$ & Recall$_0$ & Precision$_0$ \\
    \midrule
    CNN        & 5\%  & 86.8 $\rightarrow$ 81.9  & 93.85 $\rightarrow$ 91.9 \\
    CNN        & 10\% & 86.8 $\rightarrow$ 80.9  & 93.85 $\rightarrow$ 95.4 \\
    ResNet-18\footnotemark[5] & 5\%  & 94.6 $\rightarrow$ 92.8  & 96.6 $\rightarrow$ 95.5 \\
    ResNet-18\footnotemark[5] & 10\% & 94.6 $\rightarrow$ 91.5  & 96.6 $\rightarrow$ 97.3 \\
    \bottomrule
  \end{tabular}
  \end{small}
  \vspace{-0.35cm}
\end{table}

On the scratch CNN, recall drops by $5-6$ points, with modest precision changes. On the pretrained ResNet-18, recall drops by $1.8$ points at $p{=}5\%$ and by $3.1$ points at $p{=}10\%$. Precision shows mixed behavior: this is consistent with class-conditional noise: training on noisy labels (without loss correction\footnote{By "loss correction" we mean using a noise-aware training objective that explicitly accounts for label flips (class-conditional noise), typically via an estimated label-transition matrix\cite{patrini2017makingdeepneuralnetworks}. We do not apply these corrections here and train with standard cross-entropy on the observed (noisy) labels.}) makes the classifier more conservative when assigning class 0 as the resulting decision region for class 0 becomes smaller (lower recall).\cite{patrini2017makingdeepneuralnetworks}.


\paragraph{Backdoor poisoning (visible trigger).}
We apply a fixed $5{\times}5$ bright patch in the bottom-right corner to a fraction $p\in\{5\%,10\%\}$ of training images, selected uniformly across classes. Each patched sample is assigned to the target class~0, regardless of its original label (see Sec.~\ref{sec:backdoor-trigger}). The aim is to preserve normal behavior on clean inputs and induce a misclassification to class~0 whenever the patch appears.

We evaluate on two validation sets.
\begin{enumerate}[noitemsep, topsep=2pt, parsep=0pt, partopsep=0pt, leftmargin=*]
  \item a \emph{clean} set, which reflects standard performance and any drift toward the target class in normal operation
  \item a \emph{triggered} set, obtained by applying the same patch to every image. On the triggered set we report overall accuracy (to quantify collapse of normal predictions) and \textbf{Attack Success Rate (ASR)}, defined as the fraction of triggered samples predicted as class~0.
\end{enumerate}

\begin{table}[H]
  \caption{Backdoor on \textit{Imagenette-160}, clean validation (no trigger): overall accuracy (\%) and drift toward the target measured as FP$\to$0 (false positives predicted as class~0). We consider clean $\rightarrow$ poisoned.}
  \label{tab:backdoor-clean}
  \centering
  \begin{small}
    \begin{tabular}{lccc}
      \toprule
      Classifier & $p$ & Acc & FP$\to$0 \\
      \midrule
      CNN        & 5\%  & 85.6 $\rightarrow$ 83.5 & 22 $\rightarrow$ 26 \\
      CNN        & 10\% & 85.6 $\rightarrow$ 82.5 & 22 $\rightarrow$ 32 \\
      ResNet-18\footnotemark[5] & 5\%  & 91.5 $\rightarrow$ 90.7 & 13 $\rightarrow$ 32 \\
      ResNet-18\footnotemark[5] & 10\% & 91.5 $\rightarrow$ 90.8 & 13 $\rightarrow$ 20 \\
      \bottomrule
    \end{tabular}
  \end{small}
  \vspace{-0.35cm}
\end{table}

\begin{table}[H]
  \caption{Backdoor on \textit{Imagenette-160}, \emph{triggered} validation: 
  triggered accuracy (\%) and ASR (\%). Baselines are clean-trained models 
  on the triggered set}
  \label{tab:backdoor-triggered}
  \centering
  \begin{small}
    \begin{tabular}{lccc}
      \toprule
      Classifier & $p$  & Acc$_\text{trig}$ & ASR \\
      \midrule
      CNN        & 5\%  & 85.6 $\rightarrow$ 35.3 & 9.1 $\rightarrow$ \textbf{71.0} \\
      CNN        & 10\% & 85.6 $\rightarrow$ 27.9 & 9.1 $\rightarrow$ \textbf{78.5} \\
      ResNet-18  & 5\%  & 91.5 $\rightarrow$ \textbf{10.9} & 9.7 $\rightarrow$ \textbf{98.8} \\
      ResNet-18  & 10\% & 91.5 $\rightarrow$ \textbf{12.6} & 9.7 $\rightarrow$ \textbf{96.9} \\
      \bottomrule
    \end{tabular}
  \end{small}
  \vspace{-0.35cm}
\end{table}

On the clean set, overall accuracy remains nearly unchanged, especially for ResNet-18, indicating that a standard performance test may fail to reveal the presence of a backdoor attack. In practice, this could lead to real-world vulnerabilities escaping quality checks: the \textit{BadNets} attack~\cite{gu2019badnetsidentifyingvulnerabilitiesmachine}, where a traffic-sign classifier preserved high clean accuracy yet misclassified stop signs containing a small sticker trigger—posing evident risks for autonomous driving systems.

Another example is given by the \textit{WaNet} attack~\cite{nguyen2021wanetimperceptiblewarpingbased}, which achieves similar stealth by embedding imperceptible geometric distortions patterns that leave clean-set accuracy unaffected while maintaining high attack success.

Nonetheless, drift toward the target is visible: FP$\to$0 increases indicating more over-predictions of class~0 even without the trigger. Under the trigger, failure is \textbf{decisive}: ASR saturates at ${\approx}97$-$99\%$ on ResNet-18 and ${\approx}71$-$79\%$ on the CNN, with a collapse of triggered accuracy. The small ASR gap between $p{=}10\%$ and $p{=}5\%$ on ResNet-18 (96.97 vs.\ 98.80) can be explained by a near-saturation effect. We also note that FP$\to$0 is \emph{lower} at $p{=}10\%$ (20 vs.\ 32) in our run, but we may not consider it to be a general trend.

% ------------------------------------------------------------------------------
% DISCUSSION AND CONCLUSIONS
% ------------------------------------------------------------------------------
\section{Discussion and Conclusions}\label{sec:discussion}
% TODO: 5-8 bullet sintetici
% - Perché PGD > FGSM (first-order adversary)
% - Prompt injection indiretta/recursive nei workflow con tool/agent
% - Impatto del poisoning anche con basse percentuali
% - Difese pratiche essenziali (adversarial training base, input filtering, data curation)
% - Limiti del lavoro (tempo/compute) e spunti futuri (UAP, difese per LLM)

% ------------------------------------------------------------------------------
% BIBLIOGRAPHY
% ------------------------------------------------------------------------------
\bibliography{references.bib}
\bibliographystyle{dlaiml2025}

\end{document}